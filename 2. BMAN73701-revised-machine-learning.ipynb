{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using the dataset from the data preparation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/alexander/Documents/MSc Data Science/Python/Coursework/Copia de PythonCoursework/data/FINALdataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used a manual approach instead of test train split to keep the groups intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and train datasets, manually to preserve the groups\n",
    "X = dataset.drop(['Task ID','Supplier ID','Cost'], axis='columns')\n",
    "y = dataset['Cost']\n",
    "Groups = dataset['Task ID']\n",
    "\n",
    "all_tasks = Groups.unique()\n",
    "np.random.seed(42)\n",
    "TestGroup = np.random.choice(all_tasks, size=20, replace=False)\n",
    "\n",
    "test_loc = dataset['Task ID'].isin(list(TestGroup))\n",
    "\n",
    "test_ids = dataset[test_loc]['Task ID'].to_numpy()\n",
    "train_ids = dataset[~test_loc]['Task ID'].to_numpy()\n",
    "\n",
    "train_tasks = dataset['Task ID'][~test_loc]\n",
    "\n",
    "X_test = X[test_loc].to_numpy()\n",
    "y_test = y[test_loc].to_numpy()\n",
    "X_train = X[~test_loc].to_numpy()\n",
    "y_train = y[~test_loc].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a simple Lasso regression model on the training data, R2 is not very impressive but its not negative so its something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5444117174345442\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Fitting a Lasso regression model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.001)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(lasso.score(X_test,y_test))\n",
    "y_pred = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manuel suggested we should be using numpy instead of pandas for the error functions, apart from that I've tried to keep it simple\n",
    "rmse_calc is just a useful function to compute our RMSE scores, it doesn't go into the models at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function(real_costs, pred_costs):\n",
    "    real_min = real_costs.min()\n",
    "    pred_supplier = pred_costs.argmin()\n",
    "    pred_min = real_costs[pred_supplier]\n",
    "    error = real_min - pred_min\n",
    "    return error\n",
    "\n",
    "def rmse_calc(error_list):\n",
    "    error_array = np.array(error_list)\n",
    "    squared_errors = error_array*error_array\n",
    "    rss = np.sum(squared_errors)\n",
    "    value = np.sqrt(rss/len(error_array))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation is slightly more involved for the held out part, because the tasks used for testing are all together initially, so we have to split perform it for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.039830654283993294\n",
      "-0.020898605623685618\n",
      "-0.03073932084081804\n",
      "-0.019454643228496993\n",
      "-0.03314564353405608\n",
      "-0.03275120473185139\n",
      "-0.02001518678335512\n",
      "0.0\n",
      "-0.013177836730864811\n",
      "-0.019557632960402582\n",
      "-0.029904123741192667\n",
      "-0.0713267179265189\n",
      "-0.03677777223392242\n",
      "-0.04054769482851289\n",
      "-0.021350166193281728\n",
      "-0.032698786210205666\n",
      "-0.0338678548501507\n",
      "-0.013985429080132616\n",
      "-0.0212231504009443\n",
      "-0.029694603392962704\n"
     ]
    }
   ],
   "source": [
    "held_out_errors = []\n",
    "for i in TestGroup:\n",
    "    task_trues = y_test[test_ids == i]\n",
    "    task_preds = y_pred[test_ids == i]\n",
    "    held_out_errors.append(error_function(task_trues, task_preds))\n",
    "    print(error_function(task_trues, task_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't be the final error, since Alex has some changes he wants to make to feature selection, but the code should all work regardless of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031347243838081504\n"
     ]
    }
   ],
   "source": [
    "print(rmse_calc(held_out_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a scorer out of our error function, since the validation set is all one task (for each fold) it is fairly simple to apply now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "error_scorer = make_scorer(error_function)\n",
    "\n",
    "lasso_cv = Lasso(alpha=0.001)\n",
    "cv_scores = cross_val_score(lasso_cv, X_train, y_train, cv=logo, groups=train_tasks, scoring=error_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the error goes up because we're only using part of the training data in each fold, hopefully it's nothing to worry about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032439356868181835\n"
     ]
    }
   ],
   "source": [
    "print(rmse_calc(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only see predictive accuracy of any kind with tiny alpha values, but it does form a ranking which is the main thing\n",
    "Takes about 5 minutes to run for me, so bear that in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_hp = Lasso()\n",
    "params = {'alpha': np.linspace(0.001, 0.0001, 100)}\n",
    "\n",
    "grid_search = GridSearchCV(lasso_hp, param_grid=params, scoring=error_scorer, cv=logo, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train, groups=train_tasks)\n",
    "lasso_hp_results = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picks out a very very small alpha value, but at least it picks one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.00037272727272727273}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next job is report to fit the model for the chosen hyperparameters, report the rmse, copy paste the code for a new regression model, maybe MLP? and get some simple viz of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00037272727272727273"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5342701190399681\n"
     ]
    }
   ],
   "source": [
    "lasso_hp = Lasso(alpha=grid_search.best_params_['alpha'])\n",
    "lasso_hp.fit(X_train, y_train)\n",
    "print(lasso_hp.score(X_test,y_test))\n",
    "y_pred = lasso_hp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023997590206996378\n",
      "-0.01886495931830251\n",
      "-0.018176695386359243\n",
      "-0.006408969222048588\n",
      "-0.011245950423476758\n",
      "-0.0104726223705211\n",
      "-0.02762685539882359\n",
      "-0.033259263796944605\n",
      "0.0\n",
      "-0.007138839421870102\n",
      "-0.025453823409779752\n",
      "-0.0345263004921727\n",
      "-0.010408508583130804\n",
      "-0.019862313243964813\n",
      "-0.0333538663567397\n",
      "0.0\n",
      "-0.010170413261649969\n",
      "-0.0006968233468558904\n",
      "-0.05661013824413952\n",
      "-0.019587667953720522\n"
     ]
    }
   ],
   "source": [
    "hp_errors = []\n",
    "for i in TestGroup:\n",
    "    task_trues = y_test[test_ids == i]\n",
    "    task_preds = y_pred[test_ids == i]\n",
    "    hp_errors.append(error_function(task_trues, task_preds))\n",
    "    print(error_function(task_trues, task_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02302972973273373\n"
     ]
    }
   ],
   "source": [
    "print(rmse_calc(hp_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a pretty big improvement on our initital alpha choice, thank god!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're repeating the process for MLP, the code is completely symmetrical to the Lasso case except for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4319143939178349\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Fitting an MLP regression model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,100))\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_test,y_test))\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023997590206996378\n",
      "-0.032108510945115076\n",
      "-0.026527474473635015\n",
      "-0.027460291488479394\n",
      "-0.01806416970631919\n",
      "-0.06152818851905051\n",
      "-0.039079552845717014\n",
      "0.0\n",
      "-0.01019831373805774\n",
      "-0.026192222521110253\n",
      "0.0\n",
      "-0.06585817143254957\n",
      "-0.012128412404481315\n",
      "-0.019862313243964813\n",
      "-0.04338942144588592\n",
      "-0.033403086676106974\n",
      "-0.031787106479094385\n",
      "-0.015918466458471114\n",
      "-0.017629114162710013\n",
      "-0.008146861657950089\n",
      "0.030831954650411227\n"
     ]
    }
   ],
   "source": [
    "mlp_h_o_errors = []\n",
    "for i in TestGroup:\n",
    "    task_trues = y_test[test_ids == i]\n",
    "    task_preds = y_pred[test_ids == i]\n",
    "    mlp_h_o_errors.append(error_function(task_trues, task_preds))\n",
    "    print(error_function(task_trues, task_preds))\n",
    "\n",
    "print(rmse_calc(mlp_h_o_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "error_scorer = make_scorer(error_function)\n",
    "\n",
    "mlp_cv = MLPRegressor(hidden_layer_sizes=(100,100))\n",
    "cv_scores = cross_val_score(mlp_cv, X_train, y_train, cv=logo, groups=train_tasks, scoring=error_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03164548117240795\n"
     ]
    }
   ],
   "source": [
    "print(rmse_calc(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works but maybe would change the hidden layer sizes we offer, don't get a very big performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp_hp = MLPRegressor()\n",
    "params = {'hidden_layer_sizes': [(100,100),(50,50,50),(200)],\n",
    "          'solver': ['lbfgs', 'sgd', 'adam']}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(mlp_hp, param_grid=params, scoring=error_scorer, cv=logo, n_jobs=-1)\n",
    "grid_search_mlp.fit(X_train, y_train, groups=train_tasks)\n",
    "mlp_hp_results = pd.DataFrame(grid_search_mlp.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (100, 100), 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_mlp.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.529573703011494\n"
     ]
    }
   ],
   "source": [
    "mlp_hp = MLPRegressor(hidden_layer_sizes=grid_search_mlp.best_params_['hidden_layer_sizes'],\n",
    "                      solver=grid_search_mlp.best_params_['solver'])\n",
    "mlp_hp.fit(X_train, y_train)\n",
    "print(mlp_hp.score(X_test,y_test))\n",
    "y_pred = mlp_hp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.039830654283993294\n",
      "-0.020898605623685618\n",
      "-0.02598985301106721\n",
      "-0.027940593795965307\n",
      "-0.024294500817306774\n",
      "-0.0104726223705211\n",
      "-0.013258418247657333\n",
      "-0.05818814519281579\n",
      "-0.013177836730864811\n",
      "-0.014421008823887471\n",
      "-0.03870326692015297\n",
      "-0.0688667467632374\n",
      "0.0\n",
      "-0.019862313243964813\n",
      "-0.002957159628935224\n",
      "-0.04692017345022437\n",
      "-0.01276878248146518\n",
      "-0.015918466458471114\n",
      "-0.03536559377749171\n",
      "-0.014904701216639171\n"
     ]
    }
   ],
   "source": [
    "mlp_hp_errors = []\n",
    "for i in TestGroup:\n",
    "    task_trues = y_test[test_ids == i]\n",
    "    task_preds = y_pred[test_ids == i]\n",
    "    mlp_hp_errors.append(error_function(task_trues, task_preds))\n",
    "    print(error_function(task_trues, task_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030706019523182495\n"
     ]
    }
   ],
   "source": [
    "print(rmse_calc(mlp_hp_errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
