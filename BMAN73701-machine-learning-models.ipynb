{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4579e9e1-2d1d-45b7-91d6-4d72203ce624",
   "metadata": {},
   "source": [
    "# Coursework\n",
    "## Programming in Python for Business Analytics\n",
    "#### Group 19\n",
    "\n",
    "#### Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf67ac29-9b68-4db7-ab82-ee0dcc06d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ee86fb92-bb97-4434-8d18-cc3b39f14c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/alexander/Documents/MSc Data Science/Python/Coursework/Copia de PythonCoursework/data/data_py.csv') #This dataset is before correlation matrix\n",
    "tasks = pd.read_csv('/Users/alexander/Documents/MSc Data Science/Python/Coursework/tasks.csv')\n",
    "suppliers = pd.read_csv('/Users/alexander/Documents/MSc Data Science/Python/Coursework/suppliers.csv')\n",
    "cost = pd.read_csv('/Users/alexander/Documents/MSc Data Science/Python/Coursework/cost.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4253cc-6412-4e81-880d-45ec6102dc6a",
   "metadata": {},
   "source": [
    "#### 1. Training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfdda828-86d4-4a0d-ab01-50c7ff15c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and train datasets, manually to preserve the groups\n",
    "X = dataset.drop(['Task ID','Supplier ID','Cost'], axis='columns')\n",
    "y = dataset['Cost']\n",
    "Groups = dataset['Task ID']\n",
    "\n",
    "all_tasks = Groups.unique()\n",
    "random.seed(42)\n",
    "TestGroup = np.random.choice(all_tasks, size=20, replace=False)\n",
    "\n",
    "test_loc = dataset['Task ID'].isin(list(TestGroup))\n",
    "\n",
    "X_test = X[test_loc]\n",
    "y_test = y[test_loc]\n",
    "X_train = X[~test_loc]\n",
    "y_train = y[~test_loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee13d1-16e7-4a23-9f4f-b220aeeb23c9",
   "metadata": {},
   "source": [
    "#### 2. Ridge Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31d0eaff-3062-42ad-8c00-030c2f610cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Fitting a ridge regression model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge.score(X_test,y_test)\n",
    "y_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5cc0e0b6-421e-4d1c-ab26-34849a45e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Grouping by tasks and selecting a supplier for each test task\n",
    "# using the error formula provided calculate difference between true supplier and predicted supplier cost\n",
    "\n",
    "# Getting Task IDs for our predictions\n",
    "ids = dataset['Task ID'][test_loc].reset_index(drop=True)\n",
    "y_pred = pd.Series(y_pred, name=\"Predicted Cost\")\n",
    "\n",
    "y_pred_ids = pd.merge(ids, y_pred, right_index = True, left_index = True)\n",
    "pred_best_suppliers = y_pred_ids.groupby('Task ID')['Predicted Cost'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "da191012-454a-40d6-be14-7aa0d8f16b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task ID\n",
       "T102    0.418969\n",
       "T107    0.397930\n",
       "T108    0.391683\n",
       "T111    0.397508\n",
       "T12     0.450436\n",
       "T126    0.420693\n",
       "T127    0.410633\n",
       "T20     0.355949\n",
       "T23     0.321155\n",
       "T25     0.454071\n",
       "T27     0.445902\n",
       "T33     0.446751\n",
       "T54     0.381206\n",
       "T57     0.393498\n",
       "T58     0.388032\n",
       "T70     0.420737\n",
       "T79     0.420581\n",
       "T91     0.453063\n",
       "T92     0.456403\n",
       "T99     0.424343\n",
       "Name: Predicted Cost, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_best_suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4062226e-18ea-4c7a-a8bc-a647ebb17967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the true cheapest suppliers for our test set\n",
    "true_best_suppliers = cost.groupby('Task ID')['Cost'].min()\n",
    "\n",
    "test_best_suppliers = true_best_suppliers[true_best_suppliers.index.isin(TestGroup)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b3a2fb8e-01d1-4c60-824d-c7fa735a0840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task ID\n",
       "T102    0.389170\n",
       "T107    0.404848\n",
       "T108    0.375901\n",
       "T111    0.380689\n",
       "T12     0.439592\n",
       "T126    0.387038\n",
       "T127    0.394986\n",
       "T20     0.309255\n",
       "T23     0.305738\n",
       "T25     0.398989\n",
       "T27     0.403649\n",
       "T33     0.388173\n",
       "T54     0.350626\n",
       "T57     0.391735\n",
       "T58     0.357363\n",
       "T70     0.365667\n",
       "T79     0.396764\n",
       "T91     0.444910\n",
       "T92     0.416431\n",
       "T99     0.419289\n",
       "Name: Cost, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_best_suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5cae39f3-784a-42e0-bcf3-ff5ed8ccd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to calculate errors and rmse\n",
    "def error_calc(true_min_costs, predicted_min_cost):\n",
    "   error = true_min_costs - predicted_min_cost\n",
    "   return error\n",
    "\n",
    "def rmse_calc(error_array):\n",
    "    squared_errors = error_array*error_array\n",
    "    rss = np.sum(squared_errors)\n",
    "    value = np.sqrt(rss/len(error_array))\n",
    "    return value\n",
    "\n",
    "ridge_error = error_calc(test_best_suppliers, pred_best_suppliers)\n",
    "ridge_rmse = rmse_calc(ridge_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0facd553-5c37-43c1-b0b5-634197e06efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID\n",
      "T102   -0.029799\n",
      "T107    0.006918\n",
      "T108   -0.015782\n",
      "T111   -0.016819\n",
      "T12    -0.010844\n",
      "T126   -0.033656\n",
      "T127   -0.015647\n",
      "T20    -0.046694\n",
      "T23    -0.015417\n",
      "T25    -0.055082\n",
      "T27    -0.042253\n",
      "T33    -0.058577\n",
      "T54    -0.030579\n",
      "T57    -0.001763\n",
      "T58    -0.030670\n",
      "T70    -0.055070\n",
      "T79    -0.023817\n",
      "T91    -0.008153\n",
      "T92    -0.039972\n",
      "T99    -0.005054\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Results for an initial ridge model\n",
    "print(ridge_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2f8df397-0a77-4043-aaf6-ce275d66f057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03224905462971227\n"
     ]
    }
   ],
   "source": [
    "print(ridge_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ed775-b7ee-4d77-b7a5-86c7dc5d0aa0",
   "metadata": {},
   "source": [
    "#### 3. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bacd2098-20e9-42cc-b320-dc13412c10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define the custom scoring function\n",
    "def custom_error_score(y_true, y_pred, groups):\n",
    "    # Combine predictions and true values with groups (Task ID)\n",
    "    results = pd.DataFrame({'Task ID': groups, 'Actual Cost': y_true, 'Predicted Cost': y_pred})\n",
    "    \n",
    "    errors = []\n",
    "    for task_id, group in results.groupby('Task ID'):\n",
    "        # Get actual and predicted costs for this task\n",
    "        actual_costs = group['Actual Cost'].values\n",
    "        predicted_costs = group['Predicted Cost'].values\n",
    "        \n",
    "        # Identify supplier with the lowest predicted cost\n",
    "        selected_supplier_index = np.argmin(predicted_costs)\n",
    "        \n",
    "        # Calculate the error for this task\n",
    "        min_actual_cost = np.min(actual_costs)\n",
    "        actual_cost_of_selected_supplier = actual_costs[selected_supplier_index]\n",
    "        error_t = min_actual_cost - actual_cost_of_selected_supplier\n",
    "        errors.append(error_t)\n",
    "    \n",
    "    # Return the mean error across all tasks in this fold\n",
    "    return np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bae32e7e-d6d0-4774-a4dc-7cfd782b34f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the scorer\n",
    "custom_scorer = make_scorer(custom_error_score, greater_is_better=False, needs_proba=False, groups=Groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b3551138-7e70-408e-b91e-bbb508bfe01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Leave-One-Group-Out cross-validator\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "207d522c-d1a4-4e8e-a343-0d2923c636b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Leave-One-Group-Out cross-validation\n",
    "scores = []\n",
    "for train_idx, test_idx in logo.split(X, y, groups=Groups):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    groups_test = Groups.iloc[test_idx]\n",
    "    \n",
    "    # Fit the model on the training set\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = ridge.predict(X_test)\n",
    "    \n",
    "    # Calculate the score for this fold\n",
    "    fold_score = custom_error_score(y_test, y_pred, groups_test)\n",
    "    scores.append(fold_score)\n",
    "\n",
    "# Compute RMSE of the scores\n",
    "rmse_scores = np.sqrt(np.mean(np.array(scores)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f221c039-f8c1-4c39-adea-c9d4167dc420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores (Errors): [-0.0006968233468558904, 0.0, -0.0333538663567397, -0.027587412713692472, 0.0, -0.05914839017400547, -0.013923368324799734, -0.10176826978047893, -0.02527894027607369, -0.008516451896807331, -0.020098615668314668, -0.026162844294415732, -0.0157307720523448, -0.022616915102923185, -0.018320281261390403, -0.021200359395771307, -0.01589934066381954, -0.019587667953720522, -0.004074396091798493, -0.027533479518293158, -0.007138839421870102, -0.053848974085956125, -0.00990789484916299, -0.010170413261649969, -0.028250282704583518, -0.034003932973600604, -0.01362023763449266, -0.052322285247574596, -0.015682717160928394, -0.03051918766921141, -0.03744146163967027, -0.044209801174477426, -0.028386253884491897, -0.033259263796944605, -0.040530052607745504, -0.010408508583130804, -0.0010206484836585705, 0.0, -0.0015756142650044103, -0.022599907898082527, -0.007005854650330601, -0.005351169440213632, -0.012896911130141686, -0.025453823409779752, 0.0, -0.01527123789881829, 0.0, -0.013870208899441294, 0.0, -0.022370984847053077, -0.039180057435171, -0.02332472899989202, -0.011245950423476758, -0.030266195693758602, -0.024198686270085312, -0.006967245317407389, 0.0, -0.012119517287842885, -0.004631881613571143, -0.022367445911542272, 0.0, -0.0220385269670077, -0.010021299548250373, -0.019862313243964813, -0.010624667436796409, -0.011330157590503709, -0.008585813471212789, 0.0, -0.018176695386359243, -0.006444997675159103, -0.021930304986804094, -0.029468415401550707, -0.023997590206996378, -0.017244704516619414, -0.029566607456398886, -0.01886495931830251, -0.006457473355549692, -0.008421869722186837, -0.009172979233732115, -0.036678824322153525, -0.02480050172038678, -0.02528588233437118, -0.02378800863217878, -0.02047688874179382, -0.006408969222048588, -0.007634929845771687, -0.01855489368210761, -0.01660038212574161, -0.00896534444892838, 0.0, 0.0, 0.0, -0.0345263004921727, -0.03406945681060253, -0.0104726223705211, -0.020364171936651687, -0.038044923499560124, -0.027155550916456717, -0.0356850582190707, 0.0, -0.033581711709416706, -0.04218990794311239, -0.023769391018319208, -0.019270158264855564, -0.02762685539882359, -0.027172780118931483, -0.01836907712031549, -0.028818300076823178, -0.03786151620348338, -0.016541851475189995, -0.026477795918654012, -0.03147287587074732, -0.029982701797369615, -0.04247765639932782, -0.021648089370175372, -0.03766944654902432, -0.007514163007753039, -0.003929918810472277, -0.025127269604818314, -0.05661013824413952]\n"
     ]
    }
   ],
   "source": [
    "# Output the results\n",
    "print(f\"Cross-Validation Scores (Errors): {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e9f63beb-1df9-429e-88db-7547bfbb55f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of Cross-Validation Scores: 0.0255\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE of Cross-Validation Scores: {rmse_scores:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc8c954-16df-4430-b887-99eaa44f9a47",
   "metadata": {},
   "source": [
    "#### 4. Hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "81a27df6-4418-4c79-a6e2-39f9e512af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut\n",
    "\n",
    "# Custom scoring function\n",
    "def custom_error_score(estimator, X_test, y_test, groups):\n",
    "    # Predict on the test set\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    \n",
    "    # Combine predictions and true values with groups (Task ID)\n",
    "    results = pd.DataFrame({\n",
    "        'Task ID': groups,\n",
    "        'Actual Cost': y_test,\n",
    "        'Predicted Cost': y_pred\n",
    "    })\n",
    "    \n",
    "    errors = []\n",
    "    for task_id, group in results.groupby('Task ID'):\n",
    "        actual_costs = group['Actual Cost'].values\n",
    "        predicted_costs = group['Predicted Cost'].values\n",
    "        selected_supplier_index = np.argmin(predicted_costs)\n",
    "        min_actual_cost = np.min(actual_costs)\n",
    "        actual_cost_of_selected_supplier = actual_costs[selected_supplier_index]\n",
    "        error_t = min_actual_cost - actual_cost_of_selected_supplier\n",
    "        errors.append(error_t)\n",
    "    \n",
    "    # Return the mean error for this fold\n",
    "    return np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6040cc44-28b5-4424-9be0-f2f80b8df94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scorer wrapper to include `groups`\n",
    "class GroupScorer:\n",
    "    def __init__(self, groups):\n",
    "        self.groups = groups\n",
    "\n",
    "    def __call__(self, estimator, X_test, y_test):\n",
    "        return custom_error_score(estimator, X_test, y_test, self.groups)\n",
    "\n",
    "# Leave-One-Group-Out cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Hyperparameter grid for Ridge regression\n",
    "param_grid = {'alpha': [0.1, 0.5, 1, 10, 100]}\n",
    "\n",
    "# Ridge regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# GridSearch with manual scoring\n",
    "best_params = None\n",
    "best_score = None\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2cfdd08a-d575-406b-be1b-22363ca57bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, test_idx in logo.split(X, y, groups=Groups):\n",
    "    # Get train-test split\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    groups_test = Groups.iloc[test_idx]\n",
    "    \n",
    "    # Perform grid search manually for each fold\n",
    "    fold_scores = []\n",
    "    for alpha in param_grid['alpha']:\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X_train, y_train)\n",
    "        scorer = GroupScorer(groups_test)\n",
    "        score = scorer(ridge, X_test, y_test)\n",
    "        fold_scores.append(score)\n",
    "    \n",
    "    all_scores.append(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3a5f5430-0323-4561-968d-397b0e8bb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "all_scores = np.array(all_scores)\n",
    "mean_scores = np.mean(all_scores, axis=0)\n",
    "best_alpha_idx = np.argmin(mean_scores)\n",
    "best_params = {'alpha': param_grid['alpha'][best_alpha_idx]}\n",
    "best_score = mean_scores[best_alpha_idx]\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean(best_score**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c7572381-1389-48fa-820d-962b21301d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "Best RMSE from GridSearch: 0.0203\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best RMSE from GridSearch: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
