{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using the dataset from the data preparation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/charlieunsworth/Documents/Python_coursework/data_py.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used a manual approach instead of test train split to keep the groups intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and train datasets, manually to preserve the groups\n",
    "X = dataset.drop(['Task ID','Supplier ID','Cost'], axis='columns')\n",
    "y = dataset['Cost']\n",
    "Groups = dataset['Task ID']\n",
    "\n",
    "all_tasks = Groups.unique()\n",
    "np.random.seed(42)\n",
    "TestGroup = np.random.choice(all_tasks, size=20, replace=False)\n",
    "\n",
    "test_loc = dataset['Task ID'].isin(list(TestGroup))\n",
    "\n",
    "test_ids = dataset[test_loc]['Task ID'].to_numpy()\n",
    "train_ids = dataset[~test_loc]['Task ID'].to_numpy()\n",
    "\n",
    "train_tasks = dataset['Task ID'][~test_loc]\n",
    "\n",
    "X_test = X[test_loc].to_numpy()\n",
    "y_test = y[test_loc].to_numpy()\n",
    "X_train = X[~test_loc].to_numpy()\n",
    "y_train = y[~test_loc].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a simple Lasso regression model on the training data, R2 is not very impressive but its not negative so its something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5383883009865307\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Fitting a Lasso regression model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.001)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(lasso.score(X_test,y_test))\n",
    "y_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manuel suggested we should be using numpy instead of pandas for the error functions, apart from that I've tried to keep it simple\n",
    "rmse_calc is just a useful function to compute our RMSE scores, it doesn't go into the models at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function(real_costs, pred_costs):\n",
    "    real_min = real_costs.min()\n",
    "    pred_supplier = pred_costs.argmin()\n",
    "    pred_min = real_costs[pred_supplier]\n",
    "    error = real_min - pred_min\n",
    "    return error\n",
    "\n",
    "def rmse_calc(error_list):\n",
    "    error_array = np.array(error_list)\n",
    "    squared_errors = error_array*error_array\n",
    "    rss = np.sum(squared_errors)\n",
    "    value = np.sqrt(rss/len(error_array))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation is slightly more involved for the held out part, because the tasks used for testing are all together initially, so we have to split perform it for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023997590206996378\n",
      "-0.01886495931830251\n",
      "-0.018176695386359243\n",
      "-0.006408969222048588\n",
      "-0.011245950423476758\n",
      "-0.0104726223705211\n",
      "-0.02762685539882359\n",
      "-0.033259263796944605\n",
      "0.0\n",
      "-0.007138839421870102\n",
      "-0.025453823409779752\n",
      "-0.0345263004921727\n",
      "-0.010408508583130804\n",
      "-0.019862313243964813\n",
      "-0.0333538663567397\n",
      "0.0\n",
      "-0.010170413261649969\n",
      "-0.0006968233468558904\n",
      "-0.05661013824413952\n",
      "-0.019587667953720522\n"
     ]
    }
   ],
   "source": [
    "held_out_errors = []\n",
    "for i in TestGroup:\n",
    "    task_trues = y_test[test_ids == i]\n",
    "    task_preds = y_pred[test_ids == i]\n",
    "    held_out_errors.append(error_function(task_trues, task_preds))\n",
    "    print(error_function(task_trues, task_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't be the final error, since Alex has some changes he wants to make to feature selection, but the code should all work regardless of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02302972973273373\n"
     ]
    }
   ],
   "source": [
    "print(rmse_calc(held_out_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a scorer out of our error function, since the validation set is all one task (for each fold) it is fairly simple to apply now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "error_scorer = make_scorer(error_function)\n",
    "\n",
    "lasso_cv = Lasso(alpha=0.001)\n",
    "cv_scores = cross_val_score(lasso_cv, X_train, y_train, cv=logo, groups=train_tasks, scoring=error_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the error goes up because we're only using part of the training data in each fold, hopefully it's nothing to worry about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032439356868181835\n"
     ]
    }
   ],
   "source": [
    "print(rmse_calc(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only see predictive accuracy of any kind with tiny alpha values, but it does form a ranking which is the main thing\n",
    "Takes about 5 minutes to run for me, so bear that in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_hp = Lasso()\n",
    "params = {'alpha': np.linspace(0.001, 0.0001, 100)}\n",
    "\n",
    "grid_search = GridSearchCV(lasso_hp, param_grid=params, scoring=error_scorer, cv=logo, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train, groups=train_tasks)\n",
    "lasso_hp_results = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picks out a very very small alpha value, but at least it picks one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.00037272727272727273}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next job is report to fit the model for the chosen hyperparameters, report the rmse, copy paste the code for a new regression model, maybe MLP? and get some simple viz of the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
